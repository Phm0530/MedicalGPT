{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4c1f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from glob import glob\n",
    "from itertools import chain\n",
    "from typing import Optional, List, Dict, Any, Mapping\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from loguru import logger\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    "    is_torch_tpu_available,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from transformers.trainer import TRAINING_ARGS_NAME\n",
    "from transformers.utils.versions import require_version\n",
    "from transformers.integrations import is_deepspeed_zero3_enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64ce835",
   "metadata": {},
   "source": [
    "注意， 新的transformer不再支持is_torch_tpu_available，而是改成is_torch_xla_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f06050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义各种arguments。从dataclass中\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    tokenizer_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The tokenizer for weights initialization.Don't set if you want to train a model from scratch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    load_in_8bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 8bit mode or not.\"})\n",
    "    load_in_4bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 4bit mode or not.\"})\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    model_revision: Optional[str] = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    hf_hub_token: Optional[str] = field(default=None, metadata={\"help\": \"Auth token to log in with Hugging Face Hub.\"})\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n",
    "                \"dtype will be automatically derived from the model's weights.\"\n",
    "            ),\n",
    "            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n",
    "        },\n",
    "    )\n",
    "    device_map: Optional[str] = field(\n",
    "        default=\"auto\",\n",
    "        metadata={\"help\": \"Device to map model to. If `auto` is passed, the device will be selected automatically. \"},\n",
    "    )\n",
    "    trust_remote_code: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to trust remote code when loading a model from a remote checkpoint.\"},\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.model_name_or_path is None:\n",
    "            raise ValueError(\"You must specify a valid model_name_or_path to run training.\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    train_file_dir: Optional[str] = field(default=None, metadata={\"help\": \"The train text data file folder.\"})\n",
    "    validation_file_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on text file folder.\"},\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    streaming: bool = field(default=False, metadata={\"help\": \"Enable streaming mode\"})\n",
    "    block_size: Optional[int] = field(\n",
    "        default=1024,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Optional input sequence length after tokenization. \"\n",
    "                \"The training dataset will be truncated in block of this size for training. \"\n",
    "                \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    validation_split_percentage: Optional[int] = field(\n",
    "        default=1,\n",
    "        metadata={\n",
    "            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n",
    "        },\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    keep_linebreaks: bool = field(\n",
    "        default=True, metadata={\"help\": \"Whether to keep line breaks when using TXT files or not.\"}\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.streaming:\n",
    "            require_version(\"datasets>=2.0.0\", \"The streaming feature requires `datasets>=2.0.0`\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    use_peft: bool = field(default=True, metadata={\"help\": \"Whether to use peft\"})\n",
    "    target_modules: Optional[str] = field(default=\"all\")\n",
    "    lora_rank: Optional[int] = field(default=8)\n",
    "    lora_dropout: Optional[float] = field(default=0.05)\n",
    "    lora_alpha: Optional[float] = field(default=32.0)\n",
    "    modules_to_save: Optional[str] = field(default=None)\n",
    "    peft_path: Optional[str] = field(default=None)\n",
    "    qlora: bool = field(default=False, metadata={\"help\": \"Whether to use qlora\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fabfe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-9f9c5ae0-2fdb-98e2-a76f-1c6fb6586567\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"我是通义千问，阿里巴巴集团旗下的通义实验室自主研发的超大规模语言模型。我可以帮助你回答问题、创作文字，比如写故事、写公文、写邮件、写剧本、逻辑推理、编程等等，还能表达观点，玩游戏等。如果你有任何问题或需要帮助，欢迎随时告诉我！\",\"refusal\":null,\"role\":\"assistant\",\"annotations\":null,\"audio\":null,\"function_call\":null,\"tool_calls\":null}}],\"created\":1754908302,\"model\":\"qwen-plus\",\"object\":\"chat.completion\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":66,\"prompt_tokens\":26,\"total_tokens\":92,\"completion_tokens_details\":null,\"prompt_tokens_details\":{\"audio_tokens\":null,\"cached_tokens\":0}}}\n"
     ]
    }
   ],
   "source": [
    "# test Qwen Api\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n",
    "    api_key=\"sk-9dd7997e10384b23845dd69c0a7e5b19\",\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    # 模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "    model=\"qwen3-32b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"你是谁？\"},\n",
    "    ],\n",
    "    # Qwen3模型通过enable_thinking参数控制思考过程（开源版默认True，商业版默认False）\n",
    "    # 使用Qwen3开源版模型时，若未启用流式输出，请将下行取消注释，否则会报错\n",
    "    # extra_body={\"enable_thinking\": False},\n",
    ")\n",
    "print(completion.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a4ab828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_copy_and_set_values', '_get_value', '_iter', '_request_id', '_setattr_handler', 'choices', 'construct', 'copy', 'created', 'dict', 'from_orm', 'id', 'json', 'model', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'object', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'service_tier', 'system_fingerprint', 'to_dict', 'to_json', 'update_forward_refs', 'usage', 'validate']\n"
     ]
    }
   ],
   "source": [
    "print(dir(completion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d832afcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': '我是通义千问，阿里巴巴集团旗下的通义实验室自主研发的超大规模语言模型。我可以帮助你回答问题、创作文字，比如写故事、写公文、写邮件、写剧本、逻辑推理、编程等等，还能表达观点，玩游戏等。如果你有任何问题或需要帮助，欢迎随时告诉我！', 'role': 'assistant'}}]\n"
     ]
    }
   ],
   "source": [
    "print((completion.to_dict()[\"choices\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MedicalLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
